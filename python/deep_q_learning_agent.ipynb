{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'python'))\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "import pylos_env\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make('Pylos-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.95\n",
    "        self.exploration_rate = 1\n",
    "        self.memory = deque([], 200)\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    def choose_action(self, state, explore=True):\n",
    "        if explore and (random.random() < self.exploration_rate):\n",
    "            return random.randint(0, 30)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(np.array([state])))\n",
    "            \n",
    "    def create_model(self):\n",
    "        model = Sequential([Dense(200, input_shape = (96,))])\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        # Hidden layers\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "        optimizer = Adam()\n",
    "        model.compile(optimizer, loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def train(self, memory):\n",
    "        for m in memory:\n",
    "            self.memory.append(m)\n",
    "\n",
    "        num_samples = min(100, len(memory))\n",
    "        samples = random.sample(self.memory, num_samples)\n",
    "\n",
    "        states = np.array([ state for state,_,_,_,_ in samples ])\n",
    "        next_states = np.array([ next_state for _,_,_,next_state,_ in samples ])\n",
    "        rewards = np.array([ np.array([reward]) for _, _, reward, _, _ in samples ])\n",
    "        dones = np.array([ np.array([0 if done else 1])  for _, _, _, _, done in samples ])\n",
    "\n",
    "        num_samples = states.shape[0]\n",
    "        \n",
    "        q = self.model.predict(states, batch_size=min(num_samples, 50))\n",
    "        \n",
    "        update_indices = q.argmax(1)\n",
    "        updates = np.eye(31)[update_indices]\n",
    "        \n",
    "        q_next = self.model.predict(next_states, batch_size=min(num_samples, 50))\n",
    "        best_next = np.amax(q_next, 1).reshape((num_samples, 1))\n",
    "        \n",
    "        q = q * (1 - updates)\n",
    "        q = q + best_next * updates\n",
    "        \n",
    "        self.model.fit(states, q, epochs = 1, verbose=0)\n",
    "        self.exploration_rate = self.exploration_rate - 0.01\n",
    "        if self.exploration_rate < 0.05:\n",
    "            self.exploration_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [ Agent(), Agent() ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "episode 10\n",
      "episode 20\n",
      "episode 30\n",
      "episode 40\n",
      "episode 50\n",
      "episode 60\n",
      "episode 70\n",
      "episode 80\n",
      "episode 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-39bb16cf5907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# last move is always the winning move (ball on top of the pyramid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/personal/pylos-deep-q-learning/python/pylos_env/envs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_invalid_move_step_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/personal/pylos-deep-q-learning/python/pylos_env/envs.py\u001b[0m in \u001b[0;36mcreate_invalid_move_step_response\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_invalid_move_step_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_from_pylos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'invalid'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/personal/pylos-deep-q-learning/python/pylos_env/envs.py\u001b[0m in \u001b[0;36mstate_from_pylos\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mball_owner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         ]).flatten('F')\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# We'll play the game, and after each win or loss\n",
    "# we'll train 2 agents, and let them play against each other\n",
    "# Rewards: winning = 10, invalid move = -1\n",
    "# To help the agent learn valid moves more quicly, we adjust the reward with -1 for having made an invalid move\n",
    "\n",
    "# It should be possible to train without adjusting for valid/invalid moves,\n",
    "# TODO: check how much impact this has on training speed\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    #print(state)\n",
    "    done = False\n",
    "    invalid_moves = 0\n",
    "    memories = [ [], [] ]\n",
    "    while not done:\n",
    "        #print(env.pylos.current_player)\n",
    "        current = state[1]\n",
    "        #print (\"next move\", current)\n",
    "        agent = agents[current]\n",
    "        memory = memories[current]\n",
    "        \n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            reward = 10 # last move is always the winning move (ball on top of the pyramid)\n",
    "        \n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "    # train both agents with experience\n",
    "    agents[0].train(memories[0])\n",
    "    agents[1].train(memories[1])\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print (\"episode %d\"%(episode, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_weights('agent0_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one game - stop on invalid moves\n",
    "\n",
    "env = gym.envs.make('Pylos-v0')\n",
    "state = env.reset()\n",
    "for i in range(10):\n",
    "    current = state[1]\n",
    "    agent = agents[current]\n",
    "    \n",
    "    print(env.pylos.render())\n",
    "    action = agent.choose_action(state, False)\n",
    "    print (action)\n",
    "    state, _,_,_ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
